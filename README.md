# ğŸ“„ Challenge Round 1B: Persona-Driven Document Intelligence

A full-stack AI-powered application that extracts structured outlines and generates persona-driven summaries from PDF documents using a lightweight local LLaMA-based model.

---

## ğŸš€ Features

- Upload PDF files from the frontend.
- Extract **title**, **headings (H1, H2, H3)** with **page numbers**.
- Generate **personalized summaries** based on a given **persona** and **task**.
- Runs completely **offline** using the `tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf` model via `llama-cpp-python`.
- Beautiful frontend interface with animated themes, hover effects, and clean layout.

---

## ğŸ—‚ï¸ Project Structure

Challenge-1B/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app.py
â”‚ â”œâ”€â”€ pdf_utils.py
â”‚ â”œâ”€â”€ prompt_builder.py
â”‚ â”œâ”€â”€ model/
â”‚ â”‚ â””â”€â”€ tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ public/
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ components/
â”‚ â”‚ â”‚ â”œâ”€â”€ FileUpload.jsx
â”‚ â”‚ â”‚ â”œâ”€â”€ SummaryDisplay.jsx
â”‚ â”‚ â”‚ â””â”€â”€ OutlineDisplay.jsx
â”‚ â”‚ â”œâ”€â”€ App.jsx
â”‚ â”‚ â””â”€â”€ App.css
â”‚ â””â”€â”€ package.json
â””â”€â”€ README.md

yaml

---

## ğŸ§  Model Info

We use the [TinyLLaMA 1.1B Chat](https://huggingface.co/csakyo/tinyllama-1.1b-chat-v1.0-GGUF) model (`Q4_K_M` quantized GGUF format), which is:

- ğŸ”¹ Lightweight (~680MB)
- ğŸ”¹ Fully compatible with `llama-cpp-python`
- ğŸ”¹ Efficient for CPU inference
- ğŸ”¹ Ideal for offline summarization tasks

Model must be manually downloaded and placed in:

backend/model/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf

yaml

---

## âš™ï¸ Backend Setup (Flask + llama-cpp)

### 1. Install Python dependencies

```bash
cd backend
python -m venv venv
venv\Scripts\activate      # On Windows
source venv/bin/activate   # On macOS/Linux

pip install -r requirements.txt
